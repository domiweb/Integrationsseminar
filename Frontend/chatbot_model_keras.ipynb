{"cells":[{"cell_type":"markdown","metadata":{"id":"xml_ioMinPZ7"},"source":["Data Cleaning & Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FtZoQPGznRsr"},"outputs":[],"source":["import pandas as pd\n","import re\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m85UXw_SrfMD"},"outputs":[],"source":["# Auslesen der CSV-Datei \"dataset.csv\" zur weiteren Verarbeitung im Code\n","df = pd.read_csv('dataset.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wc0d3Iw5uPVK"},"outputs":[],"source":["# Ausfüllen von NAN-Werten\n","df = df.fillna('')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJecfxReuXzo"},"outputs":[],"source":["# Definition von unabhängigen Variablen und abhängigen Variablen\n","# Die unabhängigen Variablen sind in den Spalten \"feature_columns\"\n","# Die unabhängigen Variablen sind in den Spalten \"target_columns\"\n","target_columns = 'Disease'\n","feature_columns = ['Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4', 'Symptom_5', 'Symptom_6', 'Symptom_7', 'Symptom_8', 'Symptom_9',\n","                   'Symptom_10', 'Symptom_11', 'Symptom_12', 'Symptom_13', 'Symptom_14', 'Symptom_15', 'Symptom_16', 'Symptom_17']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EDYlM8oQusWg"},"outputs":[],"source":["# Lowercasing aller Werte in den target_columns (die restlichen sind alle bereits klein geschrieben)\n","def str_lower(s):\n","  return s.lower()\n","\n","df[target_columns] = df[target_columns].apply(str_lower)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Entfernung von überflüssigen Leerzeichen (da wo Leerzeichen notwendig sind wird die Funktion nicht angewendet)\n","def remove_space(s):\n","    return s.replace(' ', '')\n","\n","for col in feature_columns:\n","    df[col] = df[col].apply(remove_space)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLdPAkt2u3jc"},"outputs":[],"source":["# Ersetzen von Unterstrichen durch Leerzeichen\n","def remove_punct(s):\n","  return s.replace('_', ' ')\n","\n","for col in feature_columns:\n","  df[col] = df[col].apply(remove_punct)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mTFFh2XLvDM7"},"outputs":[],"source":["# Dataframe in JSON-Datei schreiben\n","df.to_json('symptoms.json', orient='index')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfVgyPvjvQC6"},"outputs":[],"source":["# Dataframe auslesen\n","import json\n","\n","with open('symptoms.json', 'r+') as f:\n","  symptoms = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcZsqAjsvXMX"},"outputs":[],"source":["# Alle Item in Symptoms in Listen überführen\n","sympt = []\n","for i in symptoms:\n","  sympt.append([j for j in symptoms[str(i)].values() if j != ''])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bildung der Intents\n","all_data = []\n","for s_list in sympt:\n","    tmp_dict = dict()\n","\n","    tmp_dict['tag'] = 'ask for ' + s_list[0]\n","    tmp_list = ['The patient is having ' + i for i in s_list[1:]]\n","    tmp_dict['patterns'] = tmp_list\n","    tmp_response = 'Are there further symptoms like '\n","    tmp_dict['responses'] = []\n","    tmp_loop = s_list[1:]\n","    for i, j in enumerate(tmp_loop):\n","        if i == 0:\n","            tmp_str = ', '.join(tmp_loop[i+1:])\n","        elif i>0 or i < len(tmp_loop)-1:\n","            tmp_str = ', '.join(tmp_loop[:i]) + ', '\n","            tmp_str += ', '.join(tmp_loop[i+1:])\n","        else:\n","            tmp_str = ', '.join(tmp_loop[:i-1])\n","        tmp_response += tmp_str + '?'\n","        tmp_dict['responses'].append(tmp_response)\n","        tmp_str = ''\n","        tmp_response = 'Are there further symptoms like '\n","    all_data.append(tmp_dict)\n","all_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bildung der Intents\n","for sy_list in sympt:\n","    dict_tmp = dict()\n","    dict_tmp['tag'] = 'respond for ' + sy_list[0]\n","    dict_tmp['patterns'] = []\n","    pattern_tmp = 'The patient is experiencing '\n","    loop_tmp = sy_list[1:]\n","    for i, j in enumerate(loop_tmp):\n","        if i == 0:\n","            str_tmp = ', '.join(loop_tmp[i+1:])\n","            pattern_tmp += j + ' in combination with ' + str_tmp + '.'\n","        elif i>0 or i < len(loop_tmp)-1:\n","            str_tmp = ', '.join(loop_tmp[:i]) + ', '\n","            str_tmp += ', '.join(loop_tmp[i+1:])\n","            pattern_tmp += j + ' in combination with ' + str_tmp + '.'\n","        else:\n","            str_tmp = ', '.join(loop_tmp[:i-1])\n","            pattern_tmp += j + ' in combination with ' + str_tmp + '.'\n","        dict_tmp['patterns'].append(pattern_tmp)\n","        str_tmp = ''\n","        pattern_tmp = 'The patient is experiencing '\n","    dict_tmp['responses'] = ['The patient might be sick of ' + sy_list[0]]\n","    all_data.append(dict_tmp)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bildung der Intents\n","dict_greetings = dict()\n","dict_greetings['tag'] = 'greetings'\n","dict_greetings['patterns'] = ['Hello', 'Hi there', 'Good morning', 'Good afternoon']\n","dict_greetings['responses'] = ['Hi, I\\'m Botmedix. How can I help you?', 'My name is Botmedix, what can I do for you?']\n","all_data.append(dict_greetings)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bildung der Intents\n","dict_go = dict()\n","dict_go['tag'] = 'going off'\n","dict_go['patterns'] = ['Bye', 'I\\'m going, see you!', 'Talk to you tomorrow']\n","dict_go['responses'] = ['Bye, have a nice rest of the day']\n","all_data.append(dict_go)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Bildung der Intents\n","dict_nothing = dict()\n","dict_nothing['tag'] = 'unknown'\n","dict_nothing['patterns'] = []\n","dict_nothing['responses'] = ['Sorry, I can\\'t understand you.', 'Could you repeat, please?']\n","all_data.append(dict_nothing)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Vorbereitung für den JSON-Export\n","data = dict()\n","data['intents'] = all_data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Intents in JSON-Datei schreiben\n","with open('dataset.json', 'w') as out:\n","  json.dump(data, out)"]},{"cell_type":"markdown","metadata":{"id":"5fGFQ6viw13a"},"source":["AI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2673,"status":"ok","timestamp":1679563546005,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"zc--hoUKw2N-","outputId":"e5ad6ca0-c0dd-40a1-8539-08b33d6eaa06"},"outputs":[],"source":["# Notwendige NLTK-Packages importieren und herunterladen\n","import nltk\n","\n","nltk.download('wordnet')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6333,"status":"ok","timestamp":1679563555767,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"fKmwGuMZw7UT"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","import numpy as np\n","import tensorflow as tf\n","import json\n","import pickle\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":455,"status":"ok","timestamp":1679563558667,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"DLjk5TWLh1hD"},"outputs":[],"source":["# Intents auslesen\n","import json\n","\n","with open('dataset.json', 'r+') as f:\n","  data = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2256,"status":"ok","timestamp":1679563564986,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"NcNO7fD2xPBx"},"outputs":[],"source":["# Vorbereitung des Vokabulars\n","lemmatizer = WordNetLemmatizer()\n","\n","words = []\n","classes = []\n","documents = []\n","ignore_letters = ['?', '!', '.', ',']\n","\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","\n","for intent in data['intents']:\n","  for pattern in intent['patterns']:\n","    word_list = nltk.word_tokenize(pattern)\n","    words.extend(word_list)\n","    documents.append((word_list, intent['tag']))\n","    if intent['tag'] not in classes:\n","      classes.append(intent['tag'])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3864,"status":"ok","timestamp":1679563569901,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"hEDT4sq8xsVM"},"outputs":[],"source":["# Vorbereitung des Vokabulars und Export in Pickle-Dateien\n","words = [lemmatizer.lemmatize(w) for w in words if w not in ignore_letters and w not in stop_words]\n","words = sorted(set(words))\n","classes = sorted(set(classes))\n","\n","pickle.dump(words, open('words.pkl', 'wb'))\n","pickle.dump(classes, open('classes.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data['intents'][11500]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2258,"status":"ok","timestamp":1679563574399,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"5B0pQ4O9x5ae","outputId":"9e445b6a-16d1-4aa6-9196-03676dae2520"},"outputs":[],"source":["# Durchführung des One-Hot-Encodings auf Trainingsdaten\n","training = []\n","output_empty = [0] * len(classes)\n","\n","for doc in documents:\n","  bag = []\n","  word_patterns = doc[0]\n","  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n","\n","  for word in words:\n","    bag.append(1) if word in word_patterns else bag.append(0)\n","\n","  output_row = list(output_empty)\n","  output_row[classes.index(doc[1])] = 1\n","  training.append([bag, output_row])\n","\n","random.shuffle(training)\n","training = np.array(training)\n","\n","X = list(training[:, 0])\n","y = list(training[:, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(training)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1679566504949,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"czl6UbLcyeSh"},"outputs":[],"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Activation, Dropout\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adamax, Adam\n","from tensorflow.keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48403,"status":"ok","timestamp":1679566555462,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"S5zTURswI_mq"},"outputs":[],"source":["# Bildung des neuronalen Netzes\n","model = Sequential()\n","model.add(Dense(128, input_shape = (len(X[0]),), activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(32, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(len(y[0]), activation='softmax'))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":177,"status":"ok","timestamp":1679566599443,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"Y3d3cVTaJxIa"},"outputs":[],"source":["# Bildung des neuronalen Netzes\n","# Optimierer und Callbacks instanziieren\n","adamax = Adamax()\n","early_stopping = EarlyStopping(monitor='accuracy', min_delta=1e-7, patience=20)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1679566604397,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"748FaItGJxJ-"},"outputs":[],"source":["model.compile(loss='categorical_crossentropy', optimizer=adamax, metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252895,"status":"ok","timestamp":1679566859585,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"8O8ePhfYKWQq","outputId":"07a9f5e9-87f7-410d-c407-9942b1440ddb"},"outputs":[],"source":["hist = model.fit(np.array(X), np.array(y), epochs=200*4, batch_size=8*8, verbose=1, callbacks=early_stopping)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Parameter Finetuning\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout\n","from keras.optimizers import Adamax\n","from sklearn.model_selection import GridSearchCV\n","from keras.wrappers.scikit_learn import KerasClassifier\n","\n","# Definieren der Modellarchitektur\n","def create_model(num_units1=128, dropout1=0.5, num_units2=64, dropout2=0.5,\n","                 num_units3=32, dropout3=0.5, num_units4=64, dropout4=0.5,\n","                 activation='relu', optimizer='adamax', loss='categorical_crossentropy'):\n","    model = Sequential()\n","    model.add(Dense(num_units1, input_shape=(len(X[0]),), activation=activation))\n","    model.add(Dropout(dropout1))\n","    model.add(Dense(num_units2, activation=activation))\n","    model.add(Dropout(dropout2))\n","    model.add(Dense(num_units3, activation=activation))\n","    model.add(Dropout(dropout3))\n","    model.add(Dense(num_units4, activation=activation))\n","    model.add(Dropout(dropout4))\n","    model.add(Dense(len(y[0]), activation='softmax'))\n","    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n","    return model\n","\n","# Erstellen eines KerasClassifiers mit der Modellfunktion\n","model = KerasClassifier(build_fn=create_model, verbose=0)\n","\n","# Definieren des zu durchsuchenden Hyperparametergitters\n","param_grid = {\n","    'num_units1': [64, 128, 256],\n","    'dropout1': [0.2, 0.3, 0.5],\n","    'num_units2': [32, 64, 128, 256],\n","    'dropout2': [0.2, 0.3, 0.5],\n","    'num_units3': [16, 32, 64, 128],\n","    'dropout3': [0.2, 0.3, 0.5],\n","    'num_units4': [32, 64, 128],\n","    'dropout4': [0.2, 0.3, 0.5],\n","    'activation': ['relu', 'elu', 'tanh', 'selu'],\n","    'optimizer': ['adamax', 'adam', 'sgd'],\n","    'loss' : ['categorical_crossentropy', 'binary_crossentropy', 'sparse_categorical_crossentropy']\n","}\n","\n","# Erstellen eines GridSearchCV-Objekts und Anpassung mit den Trainingsdaten\n","grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n","grid_result = grid.fit(np.array(X), np.array(y))\n","\n","# Die besten Hyperparameter ausgeben\n","print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":318,"status":"ok","timestamp":1679566876750,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"fl7STZNRKjSR"},"outputs":[],"source":["# Neuronales Netz speichern für Vorhersagen\n","model.save('botmedix.h5', hist)"]},{"cell_type":"markdown","metadata":{"id":"1bjSd5oxwwiy"},"source":["Load pretrained model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":48756,"status":"ok","timestamp":1679566930311,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"cNjtE9MEwzmO"},"outputs":[],"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","\n","lemmatizer = WordNetLemmatizer()\n","# Intents auslesen\n","with open('dataset.json', 'r+') as f:\n","  data = json.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":226,"status":"ok","timestamp":1679566940359,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"fg-OMfn3Pv0X"},"outputs":[],"source":["from tensorflow.keras.models import load_model"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":557,"status":"ok","timestamp":1679566942584,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"7Sz8OGK9Q7Ul"},"outputs":[],"source":["# Vokabular auslesen und Modell laden\n","words = pickle.load(open('words.pkl', 'rb'))\n","classes = pickle.load(open('classes.pkl', 'rb'))\n","botmedix = load_model('botmedix.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["botmedix.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14061,"status":"ok","timestamp":1679567063958,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"FW5PSvNs_yUe","outputId":"e88ce42f-6744-4b82-c955-d72f130bf241"},"outputs":[],"source":["botmedix.evaluate(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1236,"status":"ok","timestamp":1679566944852,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"vkPgSP9mRVWx"},"outputs":[],"source":["# Tokenizing des User Inputs\n","from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","stop_words.extend(['The', 'patient', 'suffering'])\n","def clean_up_sentence(sentence, stop_words=stop_words):\n","  sentence_words = nltk.word_tokenize(sentence)\n","  sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words if word not in stop_words]\n","  return sentence_words"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":188,"status":"ok","timestamp":1679566945844,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"ovbJwhmTRmNF"},"outputs":[],"source":["# Bag of Words mit den Tokens bilden\n","def bag_of_words(sentence):\n","  sentence_words = clean_up_sentence(sentence)\n","  bag = [0] * len(words)\n","  for w in sentence_words:\n","    for i, word in enumerate(words):\n","      if word == w:\n","        bag[i] = 1\n","  return np.array(bag)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1679566947933,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"lnBts_CKSB6Q"},"outputs":[],"source":["# User-Input erhalten -> Bag-of-Words berechnen -> Vorhersagen treffen und vorbereiten für die Antwort des Chatbots\n","def predict_class(sentence):\n","  bow = bag_of_words(sentence)\n","  res = botmedix.predict(np.array([bow]))[0]\n","  ERROR_THRESHOLD = 0.25\n","  results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n","\n","  results.sort(key=lambda x : x[1], reverse=True)\n","  return_list = []\n","  for r in results:\n","    return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n","  return return_list"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":316,"status":"ok","timestamp":1679566950175,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"TCL_fJudTnpL"},"outputs":[],"source":["# Antwort des Chatbots zurückgeben\n","def get_response(intents_list, intents_json):\n","  list_of_intents = intents_json['intents']\n","  if len(intents_list) > 1:\n","      tag = [i['intent'] for i in intents_list]\n","      prob = [i['probability'] for i in intents_list]\n","      result = []\n","      for i, k in enumerate(tag):\n","          for j in list_of_intents:\n","              if j['tag'] == k:\n","                  response_ = random.choice(j['responses'])\n","                  if response_.startswith('Are there further symptoms like'):\n","                    result.append(response_)\n","                  else:\n","                    result.append(response_ + ' with the probability of ' + prob[i])\n","                  break\n","  else:\n","      tag = intents_list[0]['intent']\n","      prob = intents_list[0]['probability']\n","      for i in list_of_intents:\n","          if i['tag'] == tag:\n","            response_ = random.choice(i['responses'])\n","            if response_.startswith('Are there further symptoms like'):\n","                result = response_\n","            else:\n","                result = response_ + ' with the probability of ' + prob\n","            break\n","  return result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":728},"executionInfo":{"elapsed":80967,"status":"error","timestamp":1679567031314,"user":{"displayName":"Aleksandar Trifonov","userId":"13446093907179616304"},"user_tz":-60},"id":"rvdWSngNS849","outputId":"3a0784a0-c0b9-42ef-9475-48e284b5d07c"},"outputs":[],"source":["# Testing im Backend ob es funktioniert\n","print(\"Go! Botmedix is working\")\n","\n","while True:\n","  message = input(\"\")\n","  print(message)\n","  ints = predict_class(message)\n","  res = get_response(ints, data)\n","  if type(res) == list: \n","    for r in res: print (r)\n","  if type(res) == str:print(res)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMsXET8oJ+v2zeaTIi0689p","mount_file_id":"1VNYpKENzsAZCKySX0GTVAUwYwLLzgRuq","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
